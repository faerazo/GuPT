{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project: LlamaCSE\n",
    "## Group Members:\n",
    "### Chukwudumebi Ubogu, e-mail: ubogu@student.chalmers.se\n",
    "### Francisco Alejandro Erazo Piza, e-mail: guserafr@student.gu.se\n",
    "### Nils Dunlop, e-mail: gusdunlni@student.gu.se\n",
    "### Yunyi Xu, e-mail: yunyix@student.chalmers.se"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-03 15:40:23.826802: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-01-03 15:40:23.850718: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1735915223.892817 3826111 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1735915223.905671 3826111 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-01-03 15:40:23.941468: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import json\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "from typing import List, Dict\n",
    "import numpy as np\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import numpy as np\n",
    "from typing import List, Dict, Tuple\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Data Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CourseDataProcessor:\n",
    "    def __init__(self, model_name: str = 'all-MiniLM-L6-v2'):\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "        \n",
    "    def load_json_data(self, file_path: str) -> List[Dict]:\n",
    "        with open(file_path, 'r') as f:\n",
    "            return json.load(f)\n",
    "    \n",
    "    def create_course_chunks(self, course_data: Dict) -> List[str]:\n",
    "        \"\"\"Create meaningful chunks from course data for embedding\"\"\"\n",
    "        chunks = []\n",
    "        \n",
    "        # Create a specific prerequisites chunk\n",
    "        if 'entry_requirements' in course_data:\n",
    "            prereq_chunk = f\"The prerequisites for course {course_data['course_code']} ({course_data['course_name']}) are: {course_data['entry_requirements']}\"\n",
    "            chunks.append(prereq_chunk)\n",
    "        \n",
    "        # Basic course info chunk\n",
    "        basic_info = f\"Course {course_data['course_code']}: {course_data['course_name']} is a {course_data['credits']} credit course at {course_data['department']}. {course_data['course_content']}\"\n",
    "        chunks.append(basic_info)\n",
    "        \n",
    "        # Learning outcomes chunk\n",
    "        if 'learning_outcomes' in course_data and course_data['learning_outcomes']:\n",
    "            for outcome in course_data['learning_outcomes']:\n",
    "                for category, details in outcome.items():\n",
    "                    chunks.append(f\"Learning outcome - {category}: {details}\")\n",
    "        \n",
    "        # Teaching and assessment chunks\n",
    "        if 'form_of_teaching' in course_data:\n",
    "            chunks.append(f\"Teaching format: {course_data['form_of_teaching']}\")\n",
    "        if 'assessment' in course_data:\n",
    "            chunks.append(f\"Assessment method: {course_data['assessment']}\")\n",
    "        \n",
    "        return chunks\n",
    "    \n",
    "    def generate_embeddings(self, chunks: List[str]) -> torch.Tensor:\n",
    "        \"\"\"Generate embeddings for text chunks\"\"\"\n",
    "        return self.model.encode(chunks, convert_to_tensor=True)\n",
    "    \n",
    "    def process_all_courses(self, courses_data: List[Dict]) -> Dict:\n",
    "        \"\"\"Process all courses and return embeddings with metadata\"\"\"\n",
    "        all_chunks = []\n",
    "        chunk_to_course_map = []\n",
    "        \n",
    "        for course in courses_data:\n",
    "            course_chunks = self.create_course_chunks(course)\n",
    "            all_chunks.extend(course_chunks)\n",
    "            chunk_to_course_map.extend([course['course_code']] * len(course_chunks))\n",
    "            \n",
    "        embeddings = self.generate_embeddings(all_chunks)\n",
    "        \n",
    "        return {\n",
    "            'embeddings': embeddings,\n",
    "            'chunks': all_chunks,\n",
    "            'course_map': chunk_to_course_map\n",
    "        }\n",
    "    \n",
    "    def save_embeddings(self, embeddings_data: Dict, save_path: str):\n",
    "        \"\"\"Save embeddings and metadata\"\"\"\n",
    "        torch.save({\n",
    "            'embeddings': embeddings_data['embeddings'],\n",
    "            'chunks': embeddings_data['chunks'],\n",
    "            'course_map': embeddings_data['course_map']\n",
    "        }, save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/2023/nilsdu/.local/lib/python3.9/site-packages/torch/cuda/__init__.py:716: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n"
     ]
    }
   ],
   "source": [
    "processor = CourseDataProcessor()\n",
    "courses = processor.load_json_data('data/json/merged_data.json')\n",
    "embeddings_data = processor.process_all_courses(courses)\n",
    "processor.save_embeddings(embeddings_data, 'data/embeddings/course_embeddings.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Develop RAG System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CourseRAGSystem:\n",
    "    def __init__(self, \n",
    "                 embedding_model_name: str = 'all-MiniLM-L6-v2',\n",
    "                 llm_model_name: str = 'meta-llama/Llama-3.1-8B',\n",
    "                 use_auth_token: str = os.getenv('HUGGINGFACE_TOKEN'),\n",
    "                 device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"):\n",
    "        self.embedding_model = SentenceTransformer(embedding_model_name)\n",
    "        self.embedding_model.to(device)\n",
    "        \n",
    "        # Initialize tokenizer with auth token\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\n",
    "            llm_model_name,\n",
    "            token=use_auth_token,\n",
    "            use_fast=True\n",
    "        )\n",
    "        \n",
    "        # Initialize model with auth token and proper configuration\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            llm_model_name,\n",
    "            token=use_auth_token,\n",
    "            torch_dtype=torch.float16,  # Use half precision for memory efficiency\n",
    "            device_map=\"auto\",          # Automatically handle model placement\n",
    "            load_in_8bit=True,         # Use 8-bit quantization to reduce memory usage\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "        \n",
    "    def load_embeddings(self, path: str) -> Dict:\n",
    "        \"\"\"Load saved embeddings and metadata\"\"\"\n",
    "        return torch.load(path)\n",
    "    \n",
    "    def find_relevant_chunks(self, \n",
    "                           query: str, \n",
    "                           embeddings_data: Dict,\n",
    "                           top_k: int = 3) -> List[Tuple[str, str]]:\n",
    "        \"\"\"Find most relevant text chunks for a query\"\"\"\n",
    "        # Generate embedding for the query\n",
    "        query_embedding = self.embedding_model.encode(query, convert_to_tensor=True)\n",
    "        \n",
    "        # Calculate similarities\n",
    "        similarities = torch.nn.functional.cosine_similarity(\n",
    "            query_embedding.unsqueeze(0),\n",
    "            embeddings_data['embeddings']\n",
    "        )\n",
    "        \n",
    "        # Get top-k most similar chunks\n",
    "        top_k_indices = torch.topk(similarities, k=top_k).indices\n",
    "        \n",
    "        relevant_chunks = []\n",
    "        for idx in top_k_indices:\n",
    "            chunk = embeddings_data['chunks'][idx]\n",
    "            course_code = embeddings_data['course_map'][idx]\n",
    "            relevant_chunks.append((chunk, course_code))\n",
    "            \n",
    "        return relevant_chunks\n",
    "    \n",
    "    def generate_response(self, query: str, relevant_chunks: List[Tuple[str, str]]) -> str:\n",
    "        \"\"\"Generate a response using the LLM\"\"\"\n",
    "        # Create context from relevant chunks\n",
    "        context = \"\\n\".join([f\"From course {code}: {chunk}\" for chunk, code in relevant_chunks])\n",
    "        \n",
    "        # Create a more structured prompt for Llama\n",
    "        prompt = f\"\"\"[INST] You are a helpful course information assistant. Using only the following course information:\n",
    "\n",
    "{context}\n",
    "\n",
    "Answer this question: {query}\n",
    "\n",
    "Remember to:\n",
    "1. Only use information from the provided course data\n",
    "2. If the information isn't in the provided context, say so\n",
    "3. Be clear and concise\n",
    "4. Cite the course codes when providing information [/INST]\"\"\"\n",
    "        \n",
    "        # Generate response\n",
    "        inputs = self.tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=2048)\n",
    "        inputs = {k: v.to(self.model.device) for k, v in inputs.items()}\n",
    "        \n",
    "        outputs = self.model.generate(\n",
    "            inputs['input_ids'],\n",
    "            max_length=2048,\n",
    "            num_return_sequences=1,\n",
    "            temperature=0.7,\n",
    "            top_p=0.95,\n",
    "            do_sample=True,\n",
    "            pad_token_id=self.tokenizer.pad_token_id,\n",
    "            top_k=50,\n",
    "            repetition_penalty=1.1\n",
    "        )\n",
    "        \n",
    "        response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        return response\n",
    "    \n",
    "    def query(self, question: str, embeddings_data: Dict) -> str:\n",
    "        \"\"\"Complete RAG pipeline\"\"\"\n",
    "        relevant_chunks = self.find_relevant_chunks(question, embeddings_data)\n",
    "        response = self.generate_response(question, relevant_chunks)\n",
    "        return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48c543b73aa44829bac2bb678916c200",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/tmp/ipykernel_3826111/4193974087.py:29: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(path)\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INST] You are a helpful course information assistant. Using only the following course information:\n",
      "\n",
      "From course DIT071: The prerequisites for course DIT071 (Network Security) are: The requirement for the course is to have successfully completed two years studies within the subject Computer Science or equivalent. Specifically, the course DIT420 Computer Communication or equivalent is required. A course in Computer security such as DIT641 is recommended but not required.\n",
      "From course DIT642: The prerequisites for course DIT642 (Computer Security) are: To be eligible for the course students should have successfully completed courses corresponding to 60 hec within the subject Computer Science or equivalent. A 7.5 hec course in Programming is required.\n",
      "From course DIT642: Course DIT642: Computer Security is a 7.5 credit course at Department of Computer Science and Engineering. The course gives basic knowledge in the security area, i.e. how to protect your system against intrusions and attacks. The purpose of intrusions can be to change or delete resources (data, programs, hardware, etc), to get unauthorized access to confidential information or unauthorized use of the system's services. The course covers threats and vulnerabilities in the computer systems and networks, as well as rules, methods and mechanisms for protection. Modeling and assessment of security and dependability as well as metrication methods are covered. A holistic security approach is taken and organizational, business-related, social, human, legal and ethical aspects are treated. The following topics will be covered, among others. Introduction to computer security, Overview of security threats, Introduction to cryptography, Security in operating systems, Security mechanisms including authentication, authorization, and access control, Introduction to Network Security and Intrusion detection systems, Security Models: Bell-LaPadula, Biba, Chinese wall etc., Security management, organization and ethics\n",
      "\n",
      "Answer this question: What are the prerequisites for the Computer Security course?\n",
      "\n",
      "Remember to:\n",
      "1. Only use information from the provided course data\n",
      "2. If the information isn't in the provided context, say so\n",
      "3. Be clear and concise\n",
      "4. Cite the course codes when providing information [/INST]\n"
     ]
    }
   ],
   "source": [
    "rag_system = CourseRAGSystem()\n",
    "embeddings_data = rag_system.load_embeddings('data/embeddings/course_embeddings.pt')\n",
    "\n",
    "# Test\n",
    "question = \"What are the prerequisites for the Computer Security course?\"\n",
    "response = rag_system.query(question, embeddings_data)\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
